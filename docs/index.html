---
layout: default
title: Constrained Mean Shift Using Distant Yet Related Neighbors for Representation Learning 
---
<br>
<div style="height:25px;">
<p style="text-align:center;">
  <a href="https://www.linkedin.com/in/ajinkya-tejankar/">Ajinkya Tejankar</a><sup>1,∗</sup>, 
  <a href="https://soroush-abbasi.github.io/">Soroush Abbasi Koohpayegani</a><sup>1,∗</sup>,
  <a href="#">K L Navaneet</a><sup>1,∗</sup> <br>
  , <a href="#">Kossar Pourahmadi</a><sup>1</sup>, 
  <a href="#">Akshayvarun Subramanya</a><sup>1</sup>,
  <a href="https://www.csee.umbc.edu/~hpirsiav/">Hamed Pirsiavash</a><sup>2</sup></p>
</div>
<br>
<div style="height:25px;">
<p style="text-align:center;"><sup>1</sup>University of Maryland, Baltimore County, <sup>2</sup>University of California, Davis</p>
</div>
<div style="height:30px;">
<p style="text-align:center; font-size:12px"><sup>∗</sup> denote equal contribution</p>
</div>

<div class="menu">
  <ul style="margin: 0px;">
      <li><a href='https://www.csee.umbc.edu/~hpirsiav/papers/MSF_iccv21.pdf'>[Paper]</a></li>
      <li><a href='{{ site.baseurl }}/assets/images/MSF_poster.pdf'>[Poster]</a></li>
      <li><a href='https://github.com/UCDvision/CMSF'>[Code]</a></li>
      <li><a href='/CMSF/bib.txt'>[Bib]</a></li>
  </ul>
</div>

<div>
<p style="text-align:center;"><img src="{{ site.baseurl }}/assets/images/teaser.gif" width="100%" alt style></p>

  
<h5 id="abstract"><b>Abstract</b></h5>
<p>We are interested in representation learning in self-supervised, supervised, or semi-supervised settings. 
  The prior work on applying mean-shift idea for self-supervised learning, MSF, generalizes the BYOL idea by
  pulling a query image to not only be closer to its other augmentation, but also to the nearest neighbors (NNs)
  of its other augmentation. We believe the learning can benefit from choosing far away neighbors that are still
  semantically related to the query. Hence, we propose to generalize MSF algorithm by constraining the search 
  space for nearest neighbors. We show that our method outperforms MSF in SSL setting when the constraint 
  utilizes a different augmentation of an image, and outperforms PAWS in semi-supervised setting with less 
  training resources when the constraint ensures the NNs have the same pseudo-label as the query.</p>

<h5 id="contributions"><b>Contributions</b></h5>
  <p> 
    We argue that the top-k neighbors are close to the query image by construction, and thus may not provide a strong supervision signal.
    We are interested in choosing far away (non-top) neighbors that are still semantically related to the query image. 
    This cannot be trivially achieved by increasing the number of NNs since the purity of retrieved neighbors decreases with increasing k
    , where the purity is defined as the percentage of the NNs belonging to the same semantic category as the query image.    
  </p>
  
  <p>
    We generalize MSF[15] method by simply limiting the NN search to a smaller subset that we believe is semantically related to query. 
    We define this constraint to be the NNs of another query augmentation in SSL setting and images sharing the same label or pseudo-label
    in supervised and semi-supervised settings.
  </p>
  
  <p>
    Our experiments show that the method outperforms the various baselines in all three settings with same or less amount of computation in training. 
    It outperforms MSF[15] in SSL, cross-entropy in supervised (with clean or noisy labels), and PAWS[3] in semi-supervised settings.
  </p>

  
    <p> 
   We report the total training FLOPs for forward and backward passes through the CNN backbone.
      <strong>(Left) Self-supervised:</strong> All methods are trained on ResNet-50 backbone for 200 epochs. 
      CMSF achieves competitive accuracy with considerably lower compute. <strong>(Right) Semi-supervised:</strong> 
      Circle radius is proportional to the number of GPUs/TPUs used. In addition to being compute efficient, CMSF is 
      trained with an order of magnitude lower resources, making it more practical and accessible.
  </p>

<p style="text-align:center;"><img src="{{ site.baseurl }}/assets/images/compute.png" width="100%" alt style></p>
    
  

  
  
    <h5 id="Method"><b>Method</b></h5>
<p>
      Similar to MSF[15], given a query image, we are interested in pulling its embedding closer to the mean of the
  embeddings of its nearest neighbors (NNs). However, since top NNs are close to the target itself, they may not provide a strong supervision signal. 
  On the other hand, far away (non-top) NNs may not be semantically similar to the target image. Hence, we constrain the NN search space to include 
  mostly far away points with high purity. The purity is defined as the percentage of the NNs being from the same semantic category as the query image. 
We use different constraint selection techniques to analyze our method in supervised, self- and semi-supervised settings.
    </p>
  
  
<p>
      We augment an image twice and pass them through online and target encoders followed by l<sub>2</sub> normalization to get u and v. 
  Mean-shift[15] encourages v to be close to both u and its nearest neighbors (NN). Here, we constrain the NN pool based on 
  additional knowledge in the form of supervised labels, classifier or previous augmentation based pseudo-labels. 
  These constraints ensure that the query is pulled towards semantically related NNs that are farther away from the target feature.
    </p>

<p style="text-align:center;"><img src="{{ site.baseurl }}/assets/images/teaser.gif" width="90%" alt style></p>

  <h6 id="Method"><b>Self-Supervised Settings:</b></h6>

  <p> 
    In the initial stages of learning two diverse augmentations of an image are not very close to each other in the embedding space. 
    Thus, one way of choosing far away NNs for the target u with high purity is to limit the neighbor search space based on the NNs 
    of a different augmentation u' of the target. 
  </p>
  
  
  
  <p>
    <strong> CMSF-KM:</strong>
Here, we perform clustering at the end of each epoch (using the cached embeddings of that epoch) and define 
    C to be a subset of M that shares the same cluster assignment as the target. Similar to MSF, we then use top-k 
    NNs of target u from constrained set C for loss calculation to maintain high purity. Since augmentations are chosen 
    randomly and independently at each epoch, cluster assignment and distance minimization happen with different augmentations. 
    Even though members of a cluster are close to each other in the previous epoch, the set C may not be close to the current target. 
    This improves learning by averaging distant samples with a good purity.
  </p>
  
  <p>
    <strong> CMSF-2Q:</strong>
We propose this method to show the importance of using a different augmentation to constrain the NN search space. 
In addition to M, we maintain a second memory bank M' that is exactly the same as M but containing a different (third) augmentation of the query image.
We assume w<sub>i</sub> <span>&#8712;</span> M' and u<sub>i</sub> <span>&#8712;</span> M are two embeddings corresponding to the same image x<sub>i</sub>i. Then,
    for image x<sub>i</sub>, we find NNs of w<sub>i</sub> in M' and use their indices to construct the search space C from M. 
    As a result, C will maintain good purity while being diverse.
  </p>
  
<h5 id="results"><b>Self-supervised Learning Results</b></h5>

    <p>
       We compare our model on the full ImageNet linear and nearest neighbor benchmarks using ResNet50.
      We find that given similar computational budget, our models are better than other state-of-the-art methods. 
      Our w/s variation works slightly better than the regular MSF. Note that methods with symmetric loss are not 
      directly comparable with the other ones as they need to feed each image twice though each encoder. This results
      in twice the computation for each mini-batch. One may argue that a non-symmetric BYOL with 200 epochs should be
      compared with symmetric BYOL with 100 epochs only as they use similar amount of computation. Note that symmetric 
      MoCo v2 with 400 epochs is almost the same as asymmetric MoCo v2 with 800 epochs (71.0 vs. 71.1).  
        
    </p>
<p style="text-align:center;"><img src="{{ site.baseurl }}/assets/images/result_table1.png" width="100%" alt style>
    
    </p>
    
    <p>
        We compare our model on the ImageNet 1% and 10% linear evaluation benchmarks for ResNet50. 
      The column "Fine-tuned" refers to whether the full network was fine-tuned or a single linear layer was trained. 
      Given similar computational budgets, both of our models are better than other state-of-the-art methods. 
        
    </p>
<p style="text-align:center;"><img src="{{ site.baseurl }}/assets/images/result_table2.png" width="60%" alt style>
    
    </p>
  
  
  <p>
   We compare various SSL methods on transfer tasks by training linear layers. Under similar computational budgets,
    we show that our models are consistently better or on par with other state-of-the-art methods. 
    Only a single linear layer is trained on top of features. No train time augmentations are used. 
    "rep." means we have reproduced the results using our evaluation framework for better comparison.
  </p>
  
      </p>
<p style="text-align:center;"><img src="{{ site.baseurl }}/assets/images/result_table3.png" width="100%" alt style>
    
    </p>
  
  <h5 id="evolution"><b>Evolution of Representation Over Training</b></h5>
  
    <p>
       
      We visualize the normalized features for 10 random ImageNet classes at certain epochs of MSF training. We find that over the period of training, semantic clusters are formed in the feature space.
    
    </p>
<p style="text-align:center;"><img src="{{ site.baseurl }}/assets/images/epochwise_tsne_msf_1.png" width="100%" alt style>
    
    </p>

    
    
<h5 id="cluster"><b>Nearest Neighbors Over Training</b></h5>   
    
    <p> For two random query images, we show how the nearest neighbors evolve at the learning time.
    </p>    
<p style="text-align:center;"><img src="{{ site.baseurl }}/assets/images/NN_fig.jpg" width="100%" alt style></p>
  
<h5 id="cluster"><b>Cluster Visualizations</b></h5>   
    
    <p> We cluster ImageNet dataset into 1000 clusters using k-means and show random samples from random clusters.  
      Each row corresponds to a cluster. Note that semantically similar images are clustered together.
    </p>    
<p style="text-align:center;"><img src="{{ site.baseurl }}/assets/images/clusters.jpg" width="100%" alt style></p>







<h5 id="references"><b>References</b></h5>
  <br>[1] Torchvision  models.https://pytorch.org/docs/stable/torchvision/models.html.
  <br>[2] Soroush Abbasi Koohpayegani, Ajinkya Tejankar, and Hamed Pirsiavash. Compress: Self-supervised learning by compressing representations. Advances in Neural Information Processing Systems, 33, 2020.
  <br>[3] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. InProceedings of the European Conference on Computer Vision (ECCV), pages 132–149, 2018.
  <br>[4] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In Advances in Neural Information Processing Systems, pages 9912–9924. Curran Associates, Inc., 2020.
  <br>[5] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020.
  <br>[6] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020.
  <br>[7] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. arXiv preprint arXiv:2011.10566,2020.


  <br>[8] Jean-Bastien  Grill,  Florian  Strub,  Florent  Altche,  Corentin Tallec,  Pierre  H  Richemond,  Elena  Buchatskaya,  Carl  Doersch,  Bernardo Avila Pires,  Zhaohan Daniel Guo,  Mohammad Gheshlaghi Azar,  et al.   Bootstrap your own latent:  A new  approach  to  self-supervised  learning. arXiv  preprintarXiv:2006.07733, 2020.
  <br>[9] Kaiming He,  Haoqi Fan,  Yuxin Wu,  Saining Xie,  and Ross Girshick.   Momentum  contrast  for  unsupervised  visual  representation learning.  InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9729–9738, 2020.
  <br>[10] Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. arXiv preprint arXiv:1912.01991, 2019.
  <br>[11] Ajinkya Tejankar, Soroush Abbasi Koohpayegani, Vipin Pillai, Paolo Favaro, and Hamed Pirsiavash. ISD: Self-supervised learning by iterative similarity distillation, 2020.  
  <br>[12] Feng Wang, Huaping Liu, Di Guo, and Sun Fuchun. Unsupervised representation learning by invariance propagation. In Advances in Neural Information Processing Systems, volume 33, pages 3510–3520. Curran Associates, Inc., 2020.
  <br>[13] Chen Wei, Huiyu Wang, Wei Shen, and Alan Yuille.  Co2: Consistent contrast for unsupervised visual representation learning. arXiv preprint arXiv:2010.02217, 2020.
  <br>[14] Asano YM., Rupprecht C., and Vedaldi A. Self-labelling via simultaneous clustering and representation learning. In International Conference on Learning Representations, 2020.

  
